{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': [{'role': 'user', 'content': 'Who are you?'},\n",
       "   {'role': 'assistant',\n",
       "    'content': 'I\\'m an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta'}]}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "pipe = pipeline(\"text-generation\", model=\"meta-llama/Llama-3.2-1B-Instruct\", \n",
    "                device_map='balanced_low_0')\n",
    "pipe(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': [{'role': 'user', 'content': 'Who are you?'},\n",
       "   {'role': 'assistant',\n",
       "    'content': 'I\\'m an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta'}]}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA disponible : True\n",
      "Version CUDA de PyTorch : 12.1\n",
      "Nom du GPU : NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA disponible :\", torch.cuda.is_available())  # ‚úÖ Doit afficher True\n",
    "print(\"Version CUDA de PyTorch :\", torch.version.cuda)  # ‚úÖ Doit afficher 12.1 ou 11.8 selon votre installation\n",
    "print(\"Nom du GPU :\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"Aucun GPU d√©tect√©\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Quera\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Mod√®le charg√© avec succ√®s !\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Charger le mod√®le et le tokenizer depuis le dossier o√π il a √©t√© sauvegard√©\n",
    "model_path = \"llama3-finetuned\"  # Assurez-vous que ce dossier contient le mod√®le fine-tun√©\n",
    "\n",
    "# Charger le tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Charger le mod√®le (sur GPU si dispo, sinon CPU)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path).to(device)\n",
    "\n",
    "# Mode √©valuation (important pour √©viter le calcul des gradients)\n",
    "model.eval()\n",
    "\n",
    "print(\"‚úÖ Mod√®le charg√© avec succ√®s !\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù R√©ponse g√©n√©r√©e : Quelles sont les expertises de Quera? \n",
      "\n",
      "Quera propose des services de gestion de projet, de d√©veloppement web, de d√©veloppement mobile et d'application mobile. Elle offre √©galement des services de consulting et de coaching. Quera est sp√©cialis√©e dans les domaines de la technologie, de la communication et de la formation.\n",
      "\n",
      "Quera propose des solutions personnalis√©es pour r√©pondre aux besoins sp√©cifiques de ses clients. Elle utilise des technologies avanc√©es telles que\n"
     ]
    }
   ],
   "source": [
    "# Exemple de prompt\n",
    "prompt = \"Quelles sont les expertises de Quera ?\"\n",
    "\n",
    "# Tokenisation\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# G√©n√©ration de texte\n",
    "with torch.no_grad():  # D√©sactiver le calcul des gradients\n",
    "    output = model.generate(\n",
    "        **inputs,  \n",
    "        max_length=100,  # Limite de la longueur de sortie\n",
    "        temperature=0.7,  # Plus bas = plus d√©terministe, plus haut = plus cr√©atif\n",
    "    )\n",
    "\n",
    "# D√©codage de la sortie en texte\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"üìù R√©ponse g√©n√©r√©e :\", generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Quera\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonjour, comment √ßa va? I'm doing well, thank you for asking. I'm a big fan of French cuisine and I\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Charger le tokenizer depuis le dossier o√π le mod√®le fusionn√© a √©t√© sauvegard√©\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"llama3-finetuned-merged\")\n",
    "\n",
    "# Charger le mod√®le fusionn√©\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"llama3-finetuned-merged\",\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map=\"cpu\"  # ou \"cuda\" si vous utilisez un GPU\n",
    ")\n",
    "\n",
    "# Exemple d'utilisation : g√©n√©ration de texte\n",
    "prompt = \"Bonjour, comment √ßa va ?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLFLOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "2025/02/24 12:21:52 WARNING mlflow.transformers: The model card could not be retrieved from the hub due to 404 Client Error. (Request ID: Root=1-67bc5653-09dc907b23247feb4d690627;f28c26e3-fafd-4ced-bb97-9cbe76cdb95e)\n",
      "\n",
      "Repository Not Found for url: https://huggingface.co/llama3-finetuned-merged/resolve/main/README.md.\n",
      "Please make sure you specified the correct `repo_id` and `repo_type`.\n",
      "If you are trying to access a private or gated repo, make sure you are authenticated.\n",
      "2025/02/24 12:21:52 WARNING mlflow.transformers: Unable to find license information for this model. Please verify permissible usage for the model you are storing prior to use.\n",
      "2025/02/24 12:21:52 WARNING mlflow.utils.requirements_utils: Found torch version (2.5.1+cu121) contains a local version label (+cu121). MLflow logged a pip requirement for this package as 'torch==2.5.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2025/02/24 12:21:52 WARNING mlflow.utils.requirements_utils: Found torchvision version (0.20.1+cu121) contains a local version label (+cu121). MLflow logged a pip requirement for this package as 'torchvision==0.20.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2025/02/24 12:21:52 WARNING mlflow.utils.environment: On Windows, timeout is not supported for model requirement inference. Therefore, the operation is not bound by a timeout and may hang indefinitely. If it hangs, please consider specifying the signature manually.\n",
      "2025/02/24 12:22:09 WARNING mlflow.utils.requirements_utils: Found torchvision version (0.20.1+cu121) contains a local version label (+cu121). MLflow logged a pip requirement for this package as 'torchvision==0.20.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2025/02/24 12:23:24 INFO mlflow.tracking._tracking_service.client: üèÉ View run casual-deer-887 at: http://localhost:5000/#/experiments/0/runs/4ac6d3e828034d2d8c6af645460d6f13.\n",
      "2025/02/24 12:23:24 INFO mlflow.tracking._tracking_service.client: üß™ View experiment at: http://localhost:5000/#/experiments/0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mod√®le sauvegard√© et logu√© avec MLflow !\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000/\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    model_name = \"llama3-finetuned-merged\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float32, device_map=\"cpu\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # Cr√©er une pipeline pour la g√©n√©ration de texte\n",
    "    text_generation_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "    # Log du mod√®le en pr√©cisant le type de t√¢che\n",
    "    mlflow.transformers.log_model(\n",
    "        transformers_model=text_generation_pipeline,\n",
    "        artifact_path=\"model\",\n",
    "        task=\"text-generation\"\n",
    "    )\n",
    "\n",
    "    print(\"Mod√®le sauvegard√© et logu√© avec MLflow !\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Bonjour! I'm thrilled to be here today to share with you my favorite tips and tricks for making the\"}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_generation_pipeline(\"Bonjour\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [10:59<00:00, 29.97s/it] \n",
      "2025/02/24 12:52:40 WARNING mlflow.transformers.model_io: Could not specify device parameter for this pipeline type.Falling back to loading the model with the default device.\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:02<00:00,  4.14it/s]\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "logged_model = 'runs:/563603617d64413da724367e369ad6df/model'\n",
    "\n",
    "# Load model as a PyFuncModel.\n",
    "loaded_model = mlflow.pyfunc.load_model(logged_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Bonjour! Welcome to the world of French cuisine, where elegance, sophistication, and delicious food come together in']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.predict('Bonjour') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
