{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Quera\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Quera\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Exemple de données : {'messages': [{'role': 'system', 'content': 'Vous êtes Quera, une agence spécialisée en Web et Data, située à Paris.'}, {'role': 'user', 'content': 'Quelles sont vos expertises ?'}, {'role': 'assistant', 'content': \"Nous sommes experts en création de sites WordPress, SEO, Data Science, IA et développement d'APIs.\"}]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "from peft import LoraConfig, PeftModel, get_peft_model\n",
    "from datasets import load_dataset\n",
    "\n",
    "from torch import float32\n",
    "\n",
    "\n",
    "# ✅ Charger le dataset JSONL\n",
    "dataset = load_dataset('json', data_files='data/dataset.jsonl')\n",
    "\n",
    "# ✅ Vérifier les clés du dataset\n",
    "print(\"Exemple de données :\", dataset['train'][0])  # Affiche un exemple pour vérifier sa structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'role': 'system',\n",
       "   'content': 'Vous êtes Quera, une agence spécialisée en Web et Data, située à Paris.'},\n",
       "  {'role': 'user', 'content': 'Quelles sont vos expertises ?'},\n",
       "  {'role': 'assistant',\n",
       "   'content': \"Nous sommes experts en création de sites WordPress, SEO, Data Science, IA et développement d'APIs.\"}]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10/10 [00:00<00:00, 240.50 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# ✅ Fonction de prétraitement\n",
    "def format_conversation(example):\n",
    "    return {\n",
    "        \"text\": \"\\n\".join(f\"{'Utilisateur' if msg['role'] == 'user' else 'Quera'}: {msg['content']}\" \n",
    "                           for msg in example[\"messages\"]).strip()\n",
    "    }\n",
    "\n",
    "\n",
    "# ✅ Appliquer la transformation\n",
    "dataset = dataset.map(format_conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'role': 'system',\n",
       "   'content': 'Vous êtes Quera, une agence spécialisée en Web et Data, située à Paris.'},\n",
       "  {'role': 'user', 'content': 'Quelles sont vos expertises ?'},\n",
       "  {'role': 'assistant',\n",
       "   'content': \"Nous sommes experts en création de sites WordPress, SEO, Data Science, IA et développement d'APIs.\"}],\n",
       " 'text': \"Quera: Vous êtes Quera, une agence spécialisée en Web et Data, située à Paris.\\nUtilisateur: Quelles sont vos expertises ?\\nQuera: Nous sommes experts en création de sites WordPress, SEO, Data Science, IA et développement d'APIs.\"}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # LLaMA n'a pas de token de padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|eot_id|>'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Fonction de prétraitement avec labels\n",
    "def preprocess_function(examples):\n",
    "    inputs = tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=40)\n",
    "    \n",
    "    # ✅ Copier input_ids pour labels\n",
    "    inputs[\"labels\"] = inputs[\"input_ids\"].copy()\n",
    "\n",
    "    # ✅ Remplacer les tokens de padding par -100 (Hugging Face ignore ces positions dans la perte)\n",
    "    padding_token_id = tokenizer.pad_token_id\n",
    "    inputs[\"labels\"] = [\n",
    "        [(label if label != padding_token_id else -100) for label in labels] for labels in inputs[\"labels\"]\n",
    "    ]\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function preprocess_function at 0x000001E583D86B60> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "Map: 100%|██████████| 10/10 [00:00<00:00, 218.24 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# ✅ Appliquer la transformation\n",
    "dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'role': 'system',\n",
       "   'content': 'Vous êtes Quera, une agence spécialisée en Web et Data, située à Paris.'},\n",
       "  {'role': 'user', 'content': 'Quelles sont vos expertises ?'},\n",
       "  {'role': 'assistant',\n",
       "   'content': \"Nous sommes experts en création de sites WordPress, SEO, Data Science, IA et développement d'APIs.\"}],\n",
       " 'text': \"Quera: Vous êtes Quera, une agence spécialisée en Web et Data, située à Paris.\\nUtilisateur: Quelles sont vos expertises ?\\nQuera: Nous sommes experts en création de sites WordPress, SEO, Data Science, IA et développement d'APIs.\",\n",
       " 'input_ids': [128000,\n",
       "  2232,\n",
       "  2473,\n",
       "  25,\n",
       "  41621,\n",
       "  62299,\n",
       "  3489,\n",
       "  2473,\n",
       "  11,\n",
       "  6316,\n",
       "  945,\n",
       "  768,\n",
       "  76305,\n",
       "  285,\n",
       "  8047,\n",
       "  665,\n",
       "  5000,\n",
       "  1880,\n",
       "  2956,\n",
       "  11,\n",
       "  10109,\n",
       "  8047,\n",
       "  3869,\n",
       "  12366,\n",
       "  627,\n",
       "  2810,\n",
       "  33226,\n",
       "  25,\n",
       "  3489,\n",
       "  37907,\n",
       "  15132,\n",
       "  26317,\n",
       "  6335,\n",
       "  5014,\n",
       "  18072,\n",
       "  2232,\n",
       "  2473,\n",
       "  25,\n",
       "  48109,\n",
       "  74533],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'labels': [128000,\n",
       "  2232,\n",
       "  2473,\n",
       "  25,\n",
       "  41621,\n",
       "  62299,\n",
       "  3489,\n",
       "  2473,\n",
       "  11,\n",
       "  6316,\n",
       "  945,\n",
       "  768,\n",
       "  76305,\n",
       "  285,\n",
       "  8047,\n",
       "  665,\n",
       "  5000,\n",
       "  1880,\n",
       "  2956,\n",
       "  11,\n",
       "  10109,\n",
       "  8047,\n",
       "  3869,\n",
       "  12366,\n",
       "  627,\n",
       "  2810,\n",
       "  33226,\n",
       "  25,\n",
       "  3489,\n",
       "  37907,\n",
       "  15132,\n",
       "  26317,\n",
       "  6335,\n",
       "  5014,\n",
       "  18072,\n",
       "  2232,\n",
       "  2473,\n",
       "  25,\n",
       "  48109,\n",
       "  74533]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
