{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': [{'role': 'user', 'content': 'Who are you?'},\n",
       "   {'role': 'assistant',\n",
       "    'content': 'I\\'m an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta'}]}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "pipe = pipeline(\"text-generation\", model=\"meta-llama/Llama-3.2-1B-Instruct\", \n",
    "                device_map='balanced_low_0')\n",
    "pipe(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': [{'role': 'user', 'content': 'Who are you?'},\n",
       "   {'role': 'assistant',\n",
       "    'content': 'I\\'m an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta'}]}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Qui Trump ? Quel Trump? Quel Trump? Quel Trump?\\nQui Trump? Quel Trump? Qu'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"Qui Trump ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ligne 1: OK - [INST] Quelle est la sp√©cialit√© de l'agence Quera ...\n",
      "Ligne 2: OK - [INST] O√π se situe l'agence Quera ? [/INST] L'agen...\n",
      "Ligne 3: OK - [INST] Quel est le nom du responsable de l'agence ...\n",
      "Ligne 4: OK - [INST] Quel est le mail de contact de Kevin Durant...\n",
      "Ligne 5: OK - [INST] Quels services propose Quera en d√©veloppeme...\n",
      "Ligne 6: OK - [INST] Quelles technologies utilise Quera pour le ...\n",
      "Ligne 7: OK - [INST] Quels sont les outils ma√Ætris√©s par Quera p...\n",
      "Ligne 8: OK - [INST] Quels frameworks utilise Quera pour cr√©er d...\n",
      "Ligne 9: OK - [INST] Quel framework utilise Quera pour concevoir...\n",
      "V√©rification termin√©e ‚úÖ\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# V√©rification du fichier\n",
    "file_path = \"data/dataset.jsonl\"\n",
    "\n",
    "# Lecture et v√©rification ligne par ligne\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for i, line in enumerate(f, 1):\n",
    "        try:\n",
    "            data = json.loads(line)\n",
    "            if \"text\" not in data:\n",
    "                print(f\"Ligne {i}: Cl√© 'text' manquante.\")\n",
    "            else:\n",
    "                print(f\"Ligne {i}: OK - {data['text'][:50]}...\")\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Ligne {i}: Erreur de format JSON - {e}\")\n",
    "\n",
    "print(\"V√©rification termin√©e ‚úÖ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA disponible : True\n",
      "Version CUDA de PyTorch : 12.1\n",
      "Nom du GPU : NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA disponible :\", torch.cuda.is_available())  # ‚úÖ Doit afficher True\n",
    "print(\"Version CUDA de PyTorch :\", torch.version.cuda)  # ‚úÖ Doit afficher 12.1 ou 11.8 selon votre installation\n",
    "print(\"Nom du GPU :\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"Aucun GPU d√©tect√©\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Quera\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Mod√®le charg√© avec succ√®s !\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Charger le mod√®le et le tokenizer depuis le dossier o√π il a √©t√© sauvegard√©\n",
    "model_path = \"llama3-finetuned\"  # Assurez-vous que ce dossier contient le mod√®le fine-tun√©\n",
    "\n",
    "# Charger le tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Charger le mod√®le (sur GPU si dispo, sinon CPU)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path).to(device)\n",
    "\n",
    "# Mode √©valuation (important pour √©viter le calcul des gradients)\n",
    "model.eval()\n",
    "\n",
    "print(\"‚úÖ Mod√®le charg√© avec succ√®s !\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù R√©ponse g√©n√©r√©e : Quelles sont les expertises de Quera? \n",
      "\n",
      "Quera propose des services de gestion de projet, de d√©veloppement web, de d√©veloppement mobile et d'application mobile. Elle offre √©galement des services de consulting et de coaching. Quera est sp√©cialis√©e dans les domaines de la technologie, de la communication et de la formation.\n",
      "\n",
      "Quera propose des solutions personnalis√©es pour r√©pondre aux besoins sp√©cifiques de ses clients. Elle utilise des technologies avanc√©es telles que\n"
     ]
    }
   ],
   "source": [
    "# Exemple de prompt\n",
    "prompt = \"Quelles sont les expertises de Quera ?\"\n",
    "\n",
    "# Tokenisation\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# G√©n√©ration de texte\n",
    "with torch.no_grad():  # D√©sactiver le calcul des gradients\n",
    "    output = model.generate(\n",
    "        **inputs,  \n",
    "        max_length=100,  # Limite de la longueur de sortie\n",
    "        temperature=0.7,  # Plus bas = plus d√©terministe, plus haut = plus cr√©atif\n",
    "    )\n",
    "\n",
    "# D√©codage de la sortie en texte\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"üìù R√©ponse g√©n√©r√©e :\", generated_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
