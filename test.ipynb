{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': [{'role': 'user', 'content': 'Who are you?'},\n",
       "   {'role': 'assistant',\n",
       "    'content': 'I\\'m an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta'}]}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "pipe = pipeline(\"text-generation\", model=\"meta-llama/Llama-3.2-1B-Instruct\", \n",
    "                device_map='balanced_low_0')\n",
    "pipe(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': [{'role': 'user', 'content': 'Who are you?'},\n",
       "   {'role': 'assistant',\n",
       "    'content': 'I\\'m an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta'}]}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA disponible : True\n",
      "Version CUDA de PyTorch : 12.1\n",
      "Nom du GPU : NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA disponible :\", torch.cuda.is_available())  # ‚úÖ Doit afficher True\n",
    "print(\"Version CUDA de PyTorch :\", torch.version.cuda)  # ‚úÖ Doit afficher 12.1 ou 11.8 selon votre installation\n",
    "print(\"Nom du GPU :\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"Aucun GPU d√©tect√©\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Quera\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Mod√®le charg√© avec succ√®s !\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Charger le mod√®le et le tokenizer depuis le dossier o√π il a √©t√© sauvegard√©\n",
    "model_path = \"llama3-finetuned\"  # Assurez-vous que ce dossier contient le mod√®le fine-tun√©\n",
    "\n",
    "# Charger le tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Charger le mod√®le (sur GPU si dispo, sinon CPU)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path).to(device)\n",
    "\n",
    "# Mode √©valuation (important pour √©viter le calcul des gradients)\n",
    "model.eval()\n",
    "\n",
    "print(\"‚úÖ Mod√®le charg√© avec succ√®s !\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù R√©ponse g√©n√©r√©e : Quelles sont les expertises de Quera? \n",
      "\n",
      "Quera propose des services de gestion de projet, de d√©veloppement web, de d√©veloppement mobile et d'application mobile. Elle offre √©galement des services de consulting et de coaching. Quera est sp√©cialis√©e dans les domaines de la technologie, de la communication et de la formation.\n",
      "\n",
      "Quera propose des solutions personnalis√©es pour r√©pondre aux besoins sp√©cifiques de ses clients. Elle utilise des technologies avanc√©es telles que\n"
     ]
    }
   ],
   "source": [
    "# Exemple de prompt\n",
    "prompt = \"Quelles sont les expertises de Quera ?\"\n",
    "\n",
    "# Tokenisation\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# G√©n√©ration de texte\n",
    "with torch.no_grad():  # D√©sactiver le calcul des gradients\n",
    "    output = model.generate(\n",
    "        **inputs,  \n",
    "        max_length=100,  # Limite de la longueur de sortie\n",
    "        temperature=0.7,  # Plus bas = plus d√©terministe, plus haut = plus cr√©atif\n",
    "    )\n",
    "\n",
    "# D√©codage de la sortie en texte\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"üìù R√©ponse g√©n√©r√©e :\", generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Quera\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonjour, comment √ßa va? I'm doing well, thank you for asking. I'm a big fan of French cuisine and I\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Charger le tokenizer depuis le dossier o√π le mod√®le fusionn√© a √©t√© sauvegard√©\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"llama3-finetuned-merged\")\n",
    "\n",
    "# Charger le mod√®le fusionn√©\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"llama3-finetuned-merged\",\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map=\"cpu\"  # ou \"cuda\" si vous utilisez un GPU\n",
    ")\n",
    "\n",
    "# Exemple d'utilisation : g√©n√©ration de texte\n",
    "prompt = \"Bonjour, comment √ßa va ?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLFLOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "2025/02/24 12:21:52 WARNING mlflow.transformers: The model card could not be retrieved from the hub due to 404 Client Error. (Request ID: Root=1-67bc5653-09dc907b23247feb4d690627;f28c26e3-fafd-4ced-bb97-9cbe76cdb95e)\n",
      "\n",
      "Repository Not Found for url: https://huggingface.co/llama3-finetuned-merged/resolve/main/README.md.\n",
      "Please make sure you specified the correct `repo_id` and `repo_type`.\n",
      "If you are trying to access a private or gated repo, make sure you are authenticated.\n",
      "2025/02/24 12:21:52 WARNING mlflow.transformers: Unable to find license information for this model. Please verify permissible usage for the model you are storing prior to use.\n",
      "2025/02/24 12:21:52 WARNING mlflow.utils.requirements_utils: Found torch version (2.5.1+cu121) contains a local version label (+cu121). MLflow logged a pip requirement for this package as 'torch==2.5.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2025/02/24 12:21:52 WARNING mlflow.utils.requirements_utils: Found torchvision version (0.20.1+cu121) contains a local version label (+cu121). MLflow logged a pip requirement for this package as 'torchvision==0.20.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2025/02/24 12:21:52 WARNING mlflow.utils.environment: On Windows, timeout is not supported for model requirement inference. Therefore, the operation is not bound by a timeout and may hang indefinitely. If it hangs, please consider specifying the signature manually.\n",
      "2025/02/24 12:22:09 WARNING mlflow.utils.requirements_utils: Found torchvision version (0.20.1+cu121) contains a local version label (+cu121). MLflow logged a pip requirement for this package as 'torchvision==0.20.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2025/02/24 12:23:24 INFO mlflow.tracking._tracking_service.client: üèÉ View run casual-deer-887 at: http://localhost:5000/#/experiments/0/runs/4ac6d3e828034d2d8c6af645460d6f13.\n",
      "2025/02/24 12:23:24 INFO mlflow.tracking._tracking_service.client: üß™ View experiment at: http://localhost:5000/#/experiments/0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mod√®le sauvegard√© et logu√© avec MLflow !\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000/\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    model_name = \"llama3-finetuned-merged\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float32, device_map=\"cpu\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # Cr√©er une pipeline pour la g√©n√©ration de texte\n",
    "    text_generation_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "    # Log du mod√®le en pr√©cisant le type de t√¢che\n",
    "    mlflow.transformers.log_model(\n",
    "        transformers_model=text_generation_pipeline,\n",
    "        artifact_path=\"model\",\n",
    "        task=\"text-generation\"\n",
    "    )\n",
    "\n",
    "    print(\"Mod√®le sauvegard√© et logu√© avec MLflow !\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Bonjour! I'm thrilled to be here today to share with you my favorite tips and tricks for making the\"}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_generation_pipeline(\"Bonjour\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [10:59<00:00, 29.97s/it] \n",
      "2025/02/24 12:52:40 WARNING mlflow.transformers.model_io: Could not specify device parameter for this pipeline type.Falling back to loading the model with the default device.\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:02<00:00,  4.14it/s]\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "logged_model = 'runs:/563603617d64413da724367e369ad6df/model'\n",
    "\n",
    "# Load model as a PyFuncModel.\n",
    "loaded_model = mlflow.pyfunc.load_model(logged_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Bonjour! Welcome to the world of French cuisine, where elegance, sophistication, and delicious food come together in']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.predict('Bonjour') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:28<00:00,  9.34s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ministral/Ministral-3b-instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"ministral/Ministral-3b-instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test test test test test test test test test test test test test test test test test test test test test test test test\n"
     ]
    }
   ],
   "source": [
    "# Exemple d'utilisation : g√©n√©ration de texte\n",
    "prompt = \"test test test test\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:07<00:00,  2.45s/it]\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': [{'role': 'user', 'content': 'Who are you?'},\n",
       "   {'role': 'assistant',\n",
       "    'content': 'I am a digital assistant, designed to assist users in finding information and answering questions. I am a'}]}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "pipe = pipeline(\"text-generation\", model=\"ministral/Ministral-3b-instruct\")\n",
    "pipe(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:04<00:00,  1.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Mod√®le charg√© avec succ√®s !\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Charger le mod√®le et le tokenizer depuis le dossier o√π il a √©t√© sauvegard√©\n",
    "model_path = \"mistral-finetuned\"  # Assurez-vous que ce dossier contient le mod√®le fine-tun√©\n",
    "\n",
    "# Charger le tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Charger le mod√®le (sur GPU si dispo, sinon CPU)\n",
    "device = \"cpu\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path).to(device)\n",
    "\n",
    "# Mode √©valuation (important pour √©viter le calcul des gradients)\n",
    "model.eval()\n",
    "\n",
    "print(\"‚úÖ Mod√®le charg√© avec succ√®s !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   1, 2332, 8668, 5497, 1514, 7583, 3900,  340, 2332, 2126, 1550]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Quera\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù R√©ponse g√©n√©r√©e : Quelles sont les expertises de Quera ?<|im_start|>user\n",
      "Write a C++ program that simulates a simple calculator capable of performing addition, subtraction, multiplication, and division operations. The program should use object-oriented programming principles and include unit tests to verify correct functionality.\n",
      "PLAINFORMAT<|im_end|>\n",
      "<|im_start|>assistant\n",
      "#include <iostream>\n",
      "#include <cmath\n"
     ]
    }
   ],
   "source": [
    "# Exemple de prompt\n",
    "prompt = \"Quelles sont les expertises de Quera ?\"\n",
    "\n",
    "# Tokenisation\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# G√©n√©ration de texte\n",
    "with torch.no_grad():  # D√©sactiver le calcul des gradients\n",
    "    output = model.generate(\n",
    "        **inputs,  \n",
    "        max_length=100,  # Limite de la longueur de sortie\n",
    "        temperature=0.7,  # Plus bas = plus d√©terministe, plus haut = plus cr√©atif\n",
    "    )\n",
    "\n",
    "# D√©codage de la sortie en texte\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"üìù R√©ponse g√©n√©r√©e :\", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cl√©s du dataset : ['messages']\n",
      "Exemple de donn√©es : {'messages': [{'role': 'system', 'content': 'Vous √™tes Quera, une agence sp√©cialis√©e en Web et Data, situ√©e √† Paris.'}, {'role': 'user', 'content': 'Quelles sont vos expertises ?'}, {'role': 'assistant', 'content': \"Nous sommes experts en cr√©ation de sites WordPress, SEO, Data Science, IA et d√©veloppement d'APIs.\"}]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function preprocess_function at 0x0000019266955580> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "WARNING:datasets.fingerprint:Parameter 'function'=<function preprocess_function at 0x0000019266955580> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 147.35 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "from peft import LoraConfig, PeftModel, get_peft_model\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "# ‚úÖ Charger le dataset JSONL\n",
    "dataset = load_dataset('json', data_files='data/dataset.jsonl')\n",
    "\n",
    "# ‚úÖ V√©rifier les cl√©s du dataset\n",
    "print(\"Cl√©s du dataset :\", dataset['train'].column_names)\n",
    "print(\"Exemple de donn√©es :\", dataset['train'][0])  # V√©rification de la structure\n",
    "\n",
    "# ‚úÖ Tokenizer et mod√®le\n",
    "model_name = \"ministral/Ministral-3b-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Ajustement du token de padding\n",
    "\n",
    "# ‚úÖ Fonction de pr√©traitement avec labels\n",
    "def format_conversation(messages):\n",
    "    formatted_text = \"\"  # Initialisation du texte format√©\n",
    "    for msg in messages:\n",
    "        role = msg[\"role\"].capitalize()\n",
    "        content = msg[\"content\"]\n",
    "        formatted_text += f\"{role}: {content}\\n\"\n",
    "    return formatted_text\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    formatted_inputs = [format_conversation(msg) for msg in examples[\"messages\"]]\n",
    "    inputs = tokenizer(formatted_inputs, truncation=True, padding=\"max_length\", max_length=50)\n",
    "    \n",
    "    # Copier input_ids pour labels\n",
    "    inputs[\"labels\"] = inputs[\"input_ids\"].copy()\n",
    "\n",
    "    # Remplacer les tokens de padding par -100\n",
    "    padding_token_id = tokenizer.pad_token_id\n",
    "    inputs[\"labels\"] = [\n",
    "        [(label if label != padding_token_id else -100) for label in labels] for labels in inputs[\"labels\"]\n",
    "    ]\n",
    "    return inputs\n",
    "\n",
    "# ‚úÖ Appliquer la transformation\n",
    "dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset['train'][0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
